Great project and very clean architecture. Here’s a tight pass focused only on memory management and realistic simplifications you can ship without losing your “bounded, streaming, zero-deps” goals. I’ll suggest concrete changes, with small code patches, and call out trade-offs.

---

# 1) Make the line-index counts static (easy win, ~32 KiB)

You currently pass a counts slab through the arena and thread it into every `LineBlockIdx`. That’s complexity you don’t need.

### What to change

**iosearch.h**

```diff
-typedef struct {
-    i64 block_lo; i64 block_hi;
-    i32 sub_count;
-    unsigned short* lf_counts; // length = sub_count
-    u64 gen; bool in_use;
-} LineBlockIdx;
+typedef struct {
+    i64 block_lo; i64 block_hi;
+    i32 sub_count;                    // ≤ IDX_SUB_MAX
+    unsigned short lf_counts[IDX_SUB_MAX]; // fixed, in-struct
+    u64 gen; bool in_use;
+} LineBlockIdx;

-enum Err io_open_arena2(File* io, const char* path,
-    unsigned char* search_buf, size_t search_buf_cap,
-    unsigned short* counts_slab /* IDX_MAX_BLOCKS*IDX_SUB_MAX */);
+enum Err io_open_arena2(File* io, const char* path,
+    unsigned char* search_buf, size_t search_buf_cap);
```

**iosearch.c**

```diff
-enum Err io_open_arena2(File* io, const char* path,
-    unsigned char* search_buf, size_t search_buf_cap,
-    unsigned short* counts_slab)
+enum Err io_open_arena2(File* io, const char* path,
+    unsigned char* search_buf, size_t search_buf_cap)
 {
     memset(io, 0, sizeof(*io));
-    io->arena_backed = true;
+    io->arena_backed = true; // (can keep, but no counts slab anymore)

     ...
     io->buf = search_buf;
     io->buf_cap = search_buf_cap;

-    // Initialize line index cache with arena-backed counts
+    // Initialize LRU line index cache (counts are embedded)
     io->line_idx_gen = 0;
     for (i32 i = 0; i < IDX_MAX_BLOCKS; ++i) {
         io->line_idx[i].in_use = false;
         io->line_idx[i].gen = 0;
-        io->line_idx[i].lf_counts = counts_slab + (size_t)i * (size_t)IDX_SUB_MAX;
         io->line_idx[i].sub_count = 0;
     }
```

Remove any frees of `lf_counts` in `io_close` (you already guard on `arena_backed`, but there’s nothing to free now).

**Why this helps**

* Drops one arena slice and a parameter everywhere.
* Keeps memory bounded (32 KiB per `File`), trivial cost.
* Eliminates a potential lifetime bug (dangling `lf_counts` if someone ever changed the init path).

---

# 2) Stage per-clause captures/labels on the stack (simple, robust)

You pre-sum ranges/labels across all clauses and carve pools. That saves a few `alloca`s but complicates sizing. Use stack per clause instead—cleaner and still bounded.

### What to change

**engine.c – in `engine_run`** (replace the “Set up per-clause scratch” + loop usage):

```diff
-    // Set up per-clause scratch buffers from the pools
-    Range** r_bufs = alloca(sizeof(Range*) * cc);
-    LabelWrite** l_bufs = alloca(sizeof(LabelWrite*) * cc);
-    size_t ranges_offset = 0, labels_offset = 0;
-    for (i32 i = 0; i < cc; i++) {
-        r_bufs[i] = ranges_pool + ranges_offset;
-        l_bufs[i] = labels_pool + labels_offset;
-        ranges_offset += (size_t)r_caps[i];
-        labels_offset += (size_t)l_caps[i];
-    }
-
-    // Execute each clause independently
+    // Execute each clause independently
     i32 successful_clauses = 0;
     enum Err last_err = E_OK;

     for (i32 i = 0; i < cc; i++) {
-        err = execute_clause_with_scratch(&prg->clauses[i], prg, &io, &vm, out,
-            r_bufs[i], r_caps[i],
-            l_bufs[i], l_caps[i]);
+        i32 rc, lc;
+        clause_caps(&prg->clauses[i], &rc, &lc);
+        Range*      r_tmp  = rc ? alloca((size_t)rc * sizeof *r_tmp) : NULL;
+        LabelWrite* lw_tmp = lc ? alloca((size_t)lc * sizeof *lw_tmp) : NULL;
+        err = execute_clause_with_scratch(&prg->clauses[i], prg, &io, &vm, out,
+                                          r_tmp, rc, lw_tmp, lc);
         if (err == E_OK) {
             successful_clauses++;
         } else {
             last_err = err;
         }
     }
```

Also delete the arena slices and `total_ranges/labels` maths earlier in `engine_run`.

**Why this helps**

* Removes two more arena slices and the whole “r/l pool bookkeeping.”
* Keeps worst-case memory bounded to “max `take`/`label` per clause × small structs,” living only for that clause.

---

# 3) Size regex “seen” + thread buffers from actual compiled programs (remove estimates)

In `main.c` you estimate `re_ins_estimate_max` and then size both thread lists and “seen” bitmaps. If the estimate undershoots, you can hit `E_OOM` at runtime despite having compiled fine.

Simplify: compile first, then allocate exact scratch with `malloc` (or make it static—see option 4).

### What to change

**main.c** – stop reserving thread/seen in the arena. After the “5.5) Compile regex programs” loop:

```diff
+    // 5.6) Size regex scratch from actual compiled programs (no estimates)
+    int max_nins = 0;
+    for (i32 ci = 0; ci < prg.clause_count; ++ci) {
+        Clause* clause = &prg.clauses[ci];
+        for (i32 i = 0; i < clause->op_count; ++i) {
+            Op* op = &clause->ops[i];
+            if (op->kind == OP_FINDR && op->u.findr.prog) {
+                if (op->u.findr.prog->nins > max_nins)
+                    max_nins = op->u.findr.prog->nins;
+            }
+        }
+    }
+    int re_threads_cap = max_nins > 0 ? 4 * max_nins : 32;
+    if (re_threads_cap < 32) re_threads_cap = 32;
+    ReThread* re_curr_thr = malloc((size_t)re_threads_cap * sizeof *re_curr_thr);
+    ReThread* re_next_thr = malloc((size_t)re_threads_cap * sizeof *re_next_thr);
+    unsigned char* seen_curr = malloc(max_nins > 0 ? (size_t)max_nins : 32);
+    unsigned char* seen_next = malloc(max_nins > 0 ? (size_t)max_nins : 32);
+    if (!re_curr_thr || !re_next_thr || !seen_curr || !seen_next) {
+        die(E_OOM, "regex scratch");
+        free(block);
+        return 2;
+    }
```

Then, when opening I/O:

```diff
-    e = io_open_arena2(&io, path, search_buf, search_buf_cap, counts_slab);
+    e = io_open_arena2(&io, path, search_buf, search_buf_cap);
     ...
-    io_set_regex_scratch(&io, re_curr_thr, re_next_thr, re_threads_cap,
-                         seen_curr, seen_next, (size_t)re_seen_bytes_each);
+    io_set_regex_scratch(&io, re_curr_thr, re_next_thr, re_threads_cap,
+                         seen_curr, seen_next, (size_t)(max_nins > 0 ? max_nins : 32));
```

And at the very end (before returning):

```diff
+    free(seen_next); free(seen_curr);
+    free(re_next_thr); free(re_curr_thr);
```

Also drop the arena slices + totals you previously reserved for regex scratch in `main.c`.

**Why this helps**

* Removes a preflight estimation path and the under/over-provisioning dance.
* Keeps scratch bounded and *exactly* sized to what you compiled.

---

# 4) (Optional) Embed the search buffer for a “static IO” build (0 allocations)

If you want a “no malloc at runtime” CLI build, embed the search buffer in `File`. Memory cost is the buffer size per open file (default max ≈ 8 MiB). Good for the single-file CLI case; you can leave the arena path available for library use.

### How

**iosearch.h**

```diff
-    unsigned char* buf;
-    size_t buf_cap; // allocate once, e.g., max(FW_WIN, BK_BLK + OVERLAP_MAX)
+    #ifdef FISKTA_STATIC_IOBUF
+    enum { SEARCH_BUF_CAP = (FW_WIN > (BK_BLK + OVERLAP_MAX)) ? FW_WIN : (BK_BLK + OVERLAP_MAX) };
+    unsigned char buf[SEARCH_BUF_CAP];
+    size_t buf_cap; // = SEARCH_BUF_CAP
+    #else
+    unsigned char* buf;
+    size_t buf_cap;
+    #endif
```

**iosearch.c – in `io_open_arena2`**

```diff
-    io->buf = search_buf;
-    io->buf_cap = search_buf_cap;
+    #ifdef FISKTA_STATIC_IOBUF
+    (void)search_buf; (void)search_buf_cap;
+    io->buf_cap = sizeof io->buf;
+    #else
+    io->buf = search_buf;
+    io->buf_cap = search_buf_cap;
+    #endif
```

**engine.c / main.c**: when `FISKTA_STATIC_IOBUF` is set, pass `NULL, 0` for the `search_buf` args; they will be ignored.

**Trade-off**

* **Pros:** zero heap for the IO side; simpler ownership; fewer failure modes.
* **Cons:** +8 MiB per `File`. Only enable in the CLI build (not a library used by many parallel workers).

---

# 5) (Optional) Fix names table at a sane static cap

You estimate `max_name_count` to size `Program.names`. That’s fine—but you only ever *store 32 label values* in the VM (LRU). Letting the parser have a fixed, modest cap (e.g., 128 or 256 unique label *names*) simplifies planning and removes one more preflight dimension.

**Change**

* In `Program`, always allocate `names` as `char names[MAX_NAMES][17]` (arena or static), set `MAX_NAMES` to 128 by default (tweakable via a macro). Drop the `plan->max_name_count` plumbing and the “name sizing” parts of `parse_preflight`. `find_or_add_name_build` just returns `-1` if you exceed `MAX_NAMES`.

**Trade-off**

* **Pros:** parser simpler; no OOMs from underestimated names; negligible memory (< 2 KiB @ 128).
* **Cons:** hard ceiling on unique label identifiers (but 128 is generous for a CLI).

---

# 6) Micro-tweaks / safety

* **Arena alignment helper:** `a_align(x, a)` can overflow on pathological totals. Before each `total += a_align(...);` addition, do a checked add (`if (total > SIZE_MAX - slice) fail E_OOM;`). This is defensive, costless.
* **Drop `arena_backed`?** With #1 (embedded counts) and either arena-buf or static-buf, you *never* free subparts from `io_close`. You can remove `arena_backed` entirely and state “owner frees nothing in `io_close` except the `FILE*`.” (If you foresee a malloc-owned `buf` mode, keep the flag.)

---

# 7) Alternatives (choose per build target)

* **Memory-mapped I/O (POSIX `mmap` / Win32 mapping)**: eliminates the big search buffer, simplifies segment reads, makes backward scans easy. But you’ll need a robust fallback for `stdin` (you already spool to a temp file—can `mmap` that). It’s a portability/complexity trade, not shown here since you asked for “no deps, simple.”
* **Static regex caps**: if you want a totally static build, define `RE_NINS_MAX` and `RE_THREADS_MAX` and make:

  * `unsigned char seen_curr[RE_NINS_MAX], seen_next[RE_NINS_MAX];`
  * `ReThread curr_buf[RE_THREADS_MAX], next_buf[RE_THREADS_MAX];`
    Return `E_OOM` if a compiled program exceeds caps. This removes the `malloc`s added in §3.
* **Per-clause alloca for regex “seen”**: you deliberately avoided per-search allocations; keep it that way. If you *do* consider it, only do VLAs/alloca inside the single `io_findr_window` call. I wouldn’t change this; the current long-lived scratch in `File` is the right call.

---

# 8) What this buys you

* **Fewer moving pieces:** counts slab gone, capture/label pools gone, regex scratch sized exactly once post-compile.
* **Bounded memory stays bounded:** only +32 KiB per `File` for the line counts. Everything else either stack-scoped per clause or sized by actual patterns.
* **Simpler ownership:** `io_close` only closes the `FILE*`. Arena/lifetime rules are obvious again.

---

Part 2:

Awesome—thanks for the clarity. You’re right: if you only allocate once in `main.c` and once in `engine.c`, the `arena_backed` flag and the “maybe-owned” code paths don’t earn their keep. Below is a patch set that:

1. Removes `arena_backed` and the dead ownership branch.
2. Makes `lf_counts` a fixed in-struct array (no external slab).
3. Keeps your “startup-only” allocation rule intact (no new mallocs sprinkled in loops).
4. Switches per-clause captures/labels to stack (no arena slices for them).
5. Simplifies `io_open_arena2` to always use caller-provided buffers (or the built-in static buffer variant if you enable the macro—left optional).

I kept changes minimal and mechanical so you can drop these in.

---

## Patch 1 — `iosearch.h`: static `lf_counts`, drop `arena_backed`, simplify open

```diff
*** a/src/iosearch.h
--- b/src/iosearch.h
@@
 typedef struct { int pc; i64 start; } ReThread;
@@
 typedef struct {
     i64 block_lo; // file offset of block start (aligned to IDX_BLOCK)
     i64 block_hi; // file offset of block end   (<= block_lo + IDX_BLOCK, clipped at EOF)
     i32 sub_count; // number of subchunks = ceil((block_hi - block_lo)/IDX_SUB)
-    // For each subchunk, how many LF bytes are in that subchunk
-    // uint16 is enough: max 4096 LFs per 4 KiB
-    unsigned short* lf_counts; // length = sub_count
+    // For each subchunk, how many LF bytes are in that subchunk.
+    // uint16 is enough: max 4096 LFs per 4 KiB. Fixed-size to simplify ownership.
+    unsigned short lf_counts[IDX_SUB_MAX];
     u64 gen; // for LRU
     bool in_use;
 } LineBlockIdx;
 
 typedef struct {
     FILE* f;
     i64 size;
     // one reusable buffer for searching
     unsigned char* buf;
     size_t buf_cap; // allocate once, e.g., max(FW_WIN, BK_BLK + OVERLAP_MAX)
-    bool arena_backed; // if true, buf/lf_counts are arena-owned and must not be freed
 
     // Bounded LRU cache of line indices
     LineBlockIdx line_idx[IDX_MAX_BLOCKS];
     u64 line_idx_gen;
@@
 } File;
@@
-// io_open removed - use io_open_arena2 with arena allocation instead
-enum Err io_open_arena2(File* io, const char* path,
-    unsigned char* search_buf, size_t search_buf_cap,
-    unsigned short* counts_slab /* IDX_MAX_BLOCKS*IDX_SUB_MAX */);
+// Open using caller-provided search buffer. No dynamic ownership here.
+enum Err io_open_arena2(File* io, const char* path,
+    unsigned char* search_buf, size_t search_buf_cap);
 void io_close(File* io);
 void io_reset_full(File* io);
 enum Err io_emit(File* io, i64 start, i64 end, FILE* out);
```

---

## Patch 2 — `iosearch.c`: remove counts slab & arena_backed; no frees of internal pieces

```diff
*** a/src/iosearch.c
--- b/src/iosearch.c
@@
-enum Err io_open_arena2(File* io, const char* path,
-    unsigned char* search_buf, size_t search_buf_cap,
-    unsigned short* counts_slab)
+enum Err io_open_arena2(File* io, const char* path,
+    unsigned char* search_buf, size_t search_buf_cap)
 {
     memset(io, 0, sizeof(*io));
-    io->arena_backed = true;
 
     if (strcmp(path, "-") == 0) {
         // Spool stdin to temp file
         io->f = tmpfile();
         if (!io->f)
@@
     // Wire arena-backed buffers
     io->buf = search_buf;
     io->buf_cap = search_buf_cap;
 
-    // Initialize line index cache with arena-backed counts
+    // Initialize line index cache (counts are embedded)
     io->line_idx_gen = 0;
     for (i32 i = 0; i < IDX_MAX_BLOCKS; ++i) {
         io->line_idx[i].in_use = false;
         io->line_idx[i].gen = 0;
-        io->line_idx[i].lf_counts = counts_slab + (size_t)i * (size_t)IDX_SUB_MAX;
         io->line_idx[i].sub_count = 0;
     }
@@
 void io_close(File* io)
 {
     if (io->f) {
         fclose(io->f);
         io->f = NULL;
     }
-    if (!io->arena_backed) {
-        if (io->buf) {
-            free(io->buf);
-            io->buf = NULL;
-        }
-        for (i32 i = 0; i < IDX_MAX_BLOCKS; ++i) {
-            free(io->line_idx[i].lf_counts);
-            io->line_idx[i].lf_counts = NULL;
-        }
-    }
 
     io->size = 0;
     io->buf_cap = 0;
 }
@@
 static enum Err get_line_block(File* io, i64 pos, LineBlockIdx** out)
 {
@@
-    // (re)allocate counts
     i32 sub_count = (i32)((block_hi - block_lo + IDX_SUB - 1) / IDX_SUB);
     if (sub_count <= 0)
         sub_count = 1;
     if (sub_count > IDX_SUB_MAX)
         sub_count = IDX_SUB_MAX; // defensive
     e->sub_count = sub_count;
 
-    // compute counts
+    // compute counts
     for (i32 s = 0; s < sub_count; ++s)
         e->lf_counts[s] = 0;
@@
     e->block_lo = block_lo;
     e->block_hi = block_hi;
     e->gen = ++io->line_idx_gen;
     e->in_use = true;
```

---

## Patch 3 — `engine.c`: drop counts slab + per-clause pools; stack per clause; keep “startup-only” alloc

```diff
*** a/src/engine.c
--- b/src/engine.c
@@
 enum Err engine_run(const Program* prg, const char* in_path, FILE* out)
 {
     // Calculate memory needs for arena allocation
     i32 cc = prg->clause_count;
 
-    // Calculate scratch buffer sizes for each clause
-    i32* r_caps = alloca(sizeof(i32) * cc);
-    i32* l_caps = alloca(sizeof(i32) * cc);
-    size_t total_ranges = 0;
-    size_t total_labels = 0;
-
-    for (i32 i = 0; i < cc; i++) {
-        clause_caps(&prg->clauses[i], &r_caps[i], &l_caps[i]);
-        total_ranges += (size_t)r_caps[i];
-        total_labels += (size_t)l_caps[i];
-    }
+    // We’ll allocate per-clause capture/label arrays on the stack during the loop.
 
     // Calculate total memory needed
     const size_t search_buf_cap = (FW_WIN > (BK_BLK + OVERLAP_MAX)) ? (size_t)FW_WIN : (size_t)(BK_BLK + OVERLAP_MAX);
-    const size_t counts_bytes = (size_t)IDX_MAX_BLOCKS * (size_t)IDX_SUB_MAX * sizeof(unsigned short);
-    const size_t ranges_bytes = total_ranges * sizeof(Range);
-    const size_t labels_bytes = total_labels * sizeof(LabelWrite);
+    // no counts slab, no per-clause pools here
 
@@
-    const size_t re_thr_bytes = (size_t)re_threads_cap * sizeof(ReThread);
-    const size_t re_seen_bytes_each = (size_t)(max_nins > 0 ? max_nins : 32);
+    const size_t re_thr_bytes = (size_t)re_threads_cap * sizeof(ReThread);
+    const size_t re_seen_bytes_each = (size_t)(max_nins > 0 ? max_nins : 32);
@@
-    size_t total = a_align(search_buf_cap, 1)
-        + a_align(counts_bytes, alignof(unsigned short))
-        + a_align(ranges_bytes, alignof(Range))
-        + a_align(labels_bytes, alignof(LabelWrite))
+    size_t total = a_align(search_buf_cap, 1)
         + a_align(re_thr_bytes, alignof(ReThread)) * 2
         + a_align(re_seen_bytes_each, 1) * 2
         + 64; // small cushion like main.c
@@
     unsigned char* search_buf = (unsigned char*)arena_alloc(&arena, search_buf_cap, 1);
-    unsigned short* counts_slab = (unsigned short*)arena_alloc(&arena, counts_bytes, alignof(unsigned short));
-    Range* ranges_pool = (Range*)arena_alloc(&arena, ranges_bytes, alignof(Range));
-    LabelWrite* labels_pool = (LabelWrite*)arena_alloc(&arena, labels_bytes, alignof(LabelWrite));
     ReThread* re_curr_thr = (ReThread*)arena_alloc(&arena, re_thr_bytes, alignof(ReThread));
     ReThread* re_next_thr = (ReThread*)arena_alloc(&arena, re_thr_bytes, alignof(ReThread));
     unsigned char* seen_curr = (unsigned char*)arena_alloc(&arena, re_seen_bytes_each, 1);
     unsigned char* seen_next = (unsigned char*)arena_alloc(&arena, re_seen_bytes_each, 1);
 
-    if (!search_buf || !counts_slab || !ranges_pool || !labels_pool
-        || !re_curr_thr || !re_next_thr || !seen_curr || !seen_next) {
+    if (!search_buf || !re_curr_thr || !re_next_thr || !seen_curr || !seen_next) {
         free(block);
         return E_OOM;
     }
@@
-    enum Err err = io_open_arena2(&io, in_path, search_buf, search_buf_cap, counts_slab);
+    enum Err err = io_open_arena2(&io, in_path, search_buf, search_buf_cap);
@@
-    // Set up per-clause scratch buffers from the pools
-    Range** r_bufs = alloca(sizeof(Range*) * cc);
-    LabelWrite** l_bufs = alloca(sizeof(LabelWrite*) * cc);
-
-    size_t ranges_offset = 0;
-    size_t labels_offset = 0;
-
-    for (i32 i = 0; i < cc; i++) {
-        r_bufs[i] = ranges_pool + ranges_offset;
-        l_bufs[i] = labels_pool + labels_offset;
-        ranges_offset += (size_t)r_caps[i];
-        labels_offset += (size_t)l_caps[i];
-    }
-
     // Execute each clause independently
     i32 successful_clauses = 0;
     enum Err last_err = E_OK;
 
     for (i32 i = 0; i < cc; i++) {
-        err = execute_clause_with_scratch(&prg->clauses[i], prg, &io, &vm, out,
-            r_bufs[i], r_caps[i],
-            l_bufs[i], l_caps[i]);
+        i32 rc=0, lc=0;
+        clause_caps(&prg->clauses[i], &rc, &lc);
+        Range*      r_tmp  = rc ? alloca((size_t)rc * sizeof *r_tmp) : NULL;
+        LabelWrite* lw_tmp = lc ? alloca((size_t)lc * sizeof *lw_tmp) : NULL;
+        err = execute_clause_with_scratch(&prg->clauses[i], prg, &io, &vm, out,
+                                          r_tmp, rc, lw_tmp, lc);
         if (err == E_OK) {
             successful_clauses++;
         } else {
             last_err = err;
         }
     }
```

*No functional changes elsewhere in `engine.c`. The `execute_clause_with_scratch` stays the same.*

---

## Patch 4 — `main.c`: drop counts slab + per-clause pools; keep startup-only alloc

```diff
*** a/src/main.c
--- b/src/main.c
@@
     const size_t search_buf_cap = (FW_WIN > (BK_BLK + OVERLAP_MAX)) ? (size_t)FW_WIN : (size_t)(BK_BLK + OVERLAP_MAX);
-    const size_t counts_total_u16 = (size_t)IDX_MAX_BLOCKS * (size_t)IDX_SUB_MAX;
     const size_t names_bytes = (size_t)plan.max_name_count * sizeof(char[17]);
     const size_t ops_bytes = (size_t)plan.total_ops * sizeof(Op);
     const size_t clauses_bytes = (size_t)plan.clause_count * sizeof(Clause);
     const size_t str_pool_bytes = plan.needle_bytes + (size_t)plan.needle_count; // include NULs
-    const size_t ranges_bytes = (size_t)plan.sum_take_ops * sizeof(Range);
-    const size_t labels_bytes = (size_t)plan.sum_label_ops * sizeof(LabelWrite);
+    // Per-clause ranges/labels are stack-allocated during execution
@@
-    const size_t re_threads_bytes = (size_t)re_threads_cap * sizeof(ReThread);
+    const size_t re_threads_bytes = (size_t)re_threads_cap * sizeof(ReThread);
@@
-    size_t counts_size = a_align(counts_total_u16 * sizeof(unsigned short), alignof(unsigned short));
     size_t clauses_size = a_align(clauses_bytes, alignof(Clause));
     size_t ops_size = a_align(ops_bytes, alignof(Op));
     size_t names_size = a_align(names_bytes, alignof(char));
-    size_t ranges_size = a_align(ranges_bytes, alignof(Range));
-    size_t labels_size = a_align(labels_bytes, alignof(LabelWrite));
     size_t re_prog_size = a_align(re_prog_bytes, alignof(ReProg));
     size_t re_ins_size  = a_align(re_ins_bytes,  alignof(ReInst));
     size_t re_cls_size  = a_align(re_cls_bytes,  alignof(ReClass));
     size_t str_pool_size = a_align(str_pool_bytes, alignof(char));
@@
-    size_t total = search_buf_size + counts_size + clauses_size + ops_size + names_size
-                 + ranges_size + labels_size + re_prog_size + re_ins_size + re_cls_size
+    size_t total = search_buf_size + clauses_size + ops_size + names_size
+                 + re_prog_size + re_ins_size + re_cls_size
                  + str_pool_size + re_thrbufs_size + re_seen_size + 64; // small cushion
@@
-    unsigned short* counts_slab = arena_alloc(&A, counts_total_u16 * sizeof(unsigned short), alignof(unsigned short));
     Clause* clauses_buf = arena_alloc(&A, clauses_bytes, alignof(Clause));
     Op* ops_buf = arena_alloc(&A, ops_bytes, alignof(Op));
     char(*names_buf)[17] = arena_alloc(&A, names_bytes, alignof(char));
-    Range* ranges_pool = arena_alloc(&A, ranges_bytes, alignof(Range));
-    LabelWrite* labels_pool = arena_alloc(&A, labels_bytes, alignof(LabelWrite));
     ReThread* re_curr_thr = arena_alloc(&A, re_threads_bytes, alignof(ReThread));
     ReThread* re_next_thr = arena_alloc(&A, re_threads_bytes, alignof(ReThread));
     unsigned char* seen_curr = arena_alloc(&A, re_seen_bytes_each, 1);
     unsigned char* seen_next = arena_alloc(&A, re_seen_bytes_each, 1);
     ReProg* re_progs = arena_alloc(&A, re_prog_bytes, alignof(ReProg));
     ReInst*  re_ins  = arena_alloc(&A, re_ins_bytes,  alignof(ReInst));
     ReClass* re_cls  = arena_alloc(&A, re_cls_bytes,  alignof(ReClass));
     char* str_pool = arena_alloc(&A, str_pool_bytes, alignof(char));
-    if (!search_buf || !counts_slab || !clauses_buf || !ops_buf || !names_buf || !ranges_pool || !labels_pool
+    if (!search_buf || !clauses_buf || !ops_buf || !names_buf
         || !re_curr_thr || !re_next_thr || !seen_curr || !seen_next
         || !re_progs || !re_ins || !re_cls || !str_pool) {
         die(E_OOM, "arena carve");
         free(block);
         return 2;
     }
@@
-    e = io_open_arena2(&io, path, search_buf, search_buf_cap, counts_slab);
+    e = io_open_arena2(&io, path, search_buf, search_buf_cap);
@@
-    size_t r_off = 0, l_off = 0;
     i32 ok = 0;
     enum Err last_err = E_OK;
 
     for (i32 ci = 0; ci < prg.clause_count; ++ci) {
-        i32 rc, lc;
-        clause_caps(&prg.clauses[ci], &rc, &lc);
-        Range* r = ranges_pool + r_off;
-        r_off += (size_t)rc;
-        LabelWrite* l = labels_pool + l_off;
-        l_off += (size_t)lc;
-        e = execute_clause_with_scratch(&prg.clauses[ci], &prg, &io, &vm, stdout, r, rc, l, lc);
+        i32 rc=0, lc=0;
+        clause_caps(&prg.clauses[ci], &rc, &lc);
+        Range*      r_tmp  = rc ? alloca((size_t)rc * sizeof *r_tmp) : NULL;
+        LabelWrite* lw_tmp = lc ? alloca((size_t)lc * sizeof *lw_tmp) : NULL;
+        e = execute_clause_with_scratch(&prg.clauses[ci], &prg, &io, &vm, stdout,
+                                        r_tmp, rc, lw_tmp, lc);
         if (e == E_OK)
             ok++;
         else
             last_err = e;
     }
```

---

## Why the `arena_backed` flag can go

* You never allocate/free `io->buf` or any `lf_counts` inside `iosearch.c`. The buffers are **always** supplied by the caller at startup (arena in `main.c` or `engine.c`) and live for the whole run.
* Because `io_close()` no longer frees sub-objects, there is no case where we need to know “who owns” those pointers. `io_close()` just closes the file handle and clears sizes.

That’s why this patch removes the flag and the alternate branch.

---

## Notes on the “startup-only allocations” constraint

* `main.c` and `engine.c` still do a single `malloc` (your arena block) up-front.
* Per-clause `alloca` is stack-only (not heap), and it lives just for the clause call—no fragmentation, no frees, no leaks.
* `iosearch.c` performs no allocations at all.

If you want a **completely static** IO buffer (no arena slice for `search_buf`), you can add the optional macro’d path we discussed earlier, but I left it out here to keep this patch narrowly focused and compatible with your current arena model.

---
